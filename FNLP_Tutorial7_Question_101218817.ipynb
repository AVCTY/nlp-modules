{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FNLP_Tutorial7_Question_101218817.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**COS30081 FNLP Tutorial 7: Question**\n","\n","##**Lab Task Submission at bottom #PassTask6**\n","## **Name: Arthur Vincent Chin**\n","## **Student ID: 101218817**\n"],"metadata":{"id":"vLfZ1GUDciKH"}},{"cell_type":"markdown","source":["**Pass Task Description:**\n","\n","There are several forms of LSTM that can be used. At the very basic level a variation of using LSTM would be stack LSTM layers together. A sample code is given below on how it can be easily done using keras.\n","\n","Now try to adapt either from  jupyter notebook 7b or 7c to stack 2, 3 or more layers togethers. Use the same datasets given in the notebook. Include your scripts here. Also Write down your observations (time, performance) [<50 words]\n","\n","**Additional & Optional:**\n","You can try to implement other forms of LSTM such as GRU and bidirectional LSTM as well besides stacking.\n"],"metadata":{"id":"CYnh4pbzcl1a"}},{"cell_type":"code","source":["import os\n","import re\n","import tarfile\n","\n","import requests\n","!pip install pugnlp\n","from pugnlp.futil import path_status, find_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qr0VmZur1im","executionInfo":{"status":"ok","timestamp":1650966465374,"user_tz":-480,"elapsed":9189,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"29733a17-39fb-48dc-c1b4-637e248ab269"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pugnlp in /usr/local/lib/python3.7/dist-packages (0.2.6)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from pugnlp) (6.1.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pugnlp) (3.6.0)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from pugnlp) (21.1.3)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from pugnlp) (1.0.0)\n","Requirement already satisfied: pypandoc in /usr/local/lib/python3.7/dist-packages (from pugnlp) (1.7.5)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from pugnlp) (0.37.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from pugnlp) (5.5.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pugnlp) (3.2.2)\n","Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from pugnlp) (3.7.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pugnlp) (4.64.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pugnlp) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pugnlp) (0.16.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pugnlp) (1.3.5)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pugnlp) (0.11.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pugnlp) (1.0.2)\n","Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (from pugnlp) (0.12.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pugnlp) (3.2.5)\n","Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (from pugnlp) (0.18.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim->pugnlp) (1.21.6)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->pugnlp) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pugnlp) (5.2.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->pugnlp) (5.3.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->pugnlp) (4.10.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->pugnlp) (7.7.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->pugnlp) (5.3.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->pugnlp) (5.6.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->pugnlp) (5.2.0)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->pugnlp) (5.1.1)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->pugnlp) (5.1.1)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->pugnlp) (5.5.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->pugnlp) (5.3.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (2.6.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (4.4.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->pugnlp) (0.2.5)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->pugnlp) (5.3.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->pugnlp) (0.2.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->pugnlp) (1.1.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->pugnlp) (3.6.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (4.10.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (2.15.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (4.3.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (4.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (4.11.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (21.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (5.7.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (3.8.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->pugnlp) (0.13.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->pugnlp) (2.11.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->pugnlp) (1.8.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->pugnlp) (2.8.2)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->pugnlp) (22.3.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->pugnlp) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->pugnlp) (2.0.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pugnlp) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pugnlp) (3.0.8)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pugnlp) (1.4.2)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->pugnlp) (0.4)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->pugnlp) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->pugnlp) (0.7.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->pugnlp) (0.6.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->pugnlp) (1.5.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->pugnlp) (5.0.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->pugnlp) (0.5.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pugnlp) (2022.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->pugnlp) (8.0.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->pugnlp) (1.3)\n","Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->pugnlp) (2.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->pugnlp) (21.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pugnlp) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pugnlp) (1.1.0)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pugnlp/constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n","  [datetime.datetime, pd.datetime, pd.Timestamp])\n","/usr/local/lib/python3.7/dist-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n","  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n"]}]},{"cell_type":"code","source":["# From the nlpia package for downloading data too big for the repo\n","import tqdm\n","BIG_URLS = {\n","    'w2v': (\n","        'https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1',\n","        1647046227,\n","    ),\n","    'slang': (\n","        'https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1',\n","        117633024,\n","    ),\n","    'tweets': (\n","        'https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1',\n","        311725313,\n","    ),\n","    'lsa_tweets': (\n","        'https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1',\n","        3112841563,  # 3112841312,\n","    ),\n","    'imdb': (\n","        'https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1',\n","        3112841563,  # 3112841312,\n","    ),\n","}"],"metadata":{"id":"0Kr7Cu-Fsl2u","executionInfo":{"status":"ok","timestamp":1650966473723,"user_tz":-480,"elapsed":6,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# These functions are part of the nlpia package which can be pip installed and run from there.\n","def dropbox_basename(url):\n","    filename = os.path.basename(url)\n","    match = re.findall(r'\\?dl=[0-9]$', filename)\n","    if match:\n","        return filename[:-len(match[0])]\n","    return filename\n","\n","def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n","    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n","    if filename is None:\n","        filename = dropbox_basename(url)\n","    file_path = os.path.join(data_path, filename)\n","    if url.endswith('?dl=0'):\n","        url = url[:-1] + '1'  # noninteractive download\n","    if verbose:\n","        tqdm_prog = tqdm\n","        print('requesting URL: {}'.format(url))\n","    else:\n","        tqdm_prog = no_tqdm\n","    r = requests.get(url, stream=True, allow_redirects=True)\n","    size = r.headers.get('Content-Length', None) if size is None else size\n","    print('remote size: {}'.format(size))\n","\n","    stat = path_status(file_path)\n","    print('local size: {}'.format(stat.get('size', None)))\n","    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n","        r.close()\n","        return file_path\n","\n","    print('Downloading to {}'.format(file_path))\n","\n","    with open(file_path, 'wb') as f:\n","        for chunk in r.iter_content(chunk_size=chunk_size):\n","            if chunk:  # filter out keep-alive chunks\n","                f.write(chunk)\n","\n","    r.close()\n","    return file_path\n","\n","def untar(fname):\n","    if fname.endswith(\"tar.gz\"):\n","        with tarfile.open(fname) as tf:\n","            tf.extractall()\n","    else:\n","        print(\"Not a tar.gz file: {}\".format(fname))"],"metadata":{"id":"ozXpABpdsl_I","executionInfo":{"status":"ok","timestamp":1650966475822,"user_tz":-480,"elapsed":7,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#  UNCOMMENT these 2 lines if you haven't already download the word2vec model and the imdb dataset\n","download_file(BIG_URLS['w2v'][0])\n","untar(download_file(BIG_URLS['imdb'][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6CjApo2sr4u","executionInfo":{"status":"ok","timestamp":1650966706029,"user_tz":-480,"elapsed":226911,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"879ba985-2e46-4f83-f163-88298a5fa8c4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n","remote size: 1647046227\n","local size: 1647046227\n","Downloading to ./GoogleNews-vectors-negative300.bin.gz\n","requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n","remote size: 84125825\n","local size: 84125825\n","Downloading to ./aclImdb_v1.tar.gz\n"]}]},{"cell_type":"code","source":["import glob\n","import os\n","\n","from random import shuffle\n","\n","def pre_process_data(filepath):\n","    \"\"\"\n","    This is dependent on your training data source but we will try to generalize it as best as possible.\n","    \"\"\"\n","    positive_path = os.path.join(filepath, 'pos')\n","    negative_path = os.path.join(filepath, 'neg')\n","    \n","    pos_label = 1\n","    neg_label = 0\n","    \n","    dataset = []\n","    \n","    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n","        with open(filename, 'r') as f:\n","            dataset.append((pos_label, f.read()))\n","            \n","    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n","        with open(filename, 'r') as f:\n","            dataset.append((neg_label, f.read()))\n","    \n","    shuffle(dataset)\n","    \n","    return dataset"],"metadata":{"id":"Uk9-nYNDssFP","executionInfo":{"status":"ok","timestamp":1650966706030,"user_tz":-480,"elapsed":22,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","from gensim.models import KeyedVectors\n","word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n","\n","def tokenize_and_vectorize(dataset):\n","    tokenizer = TreebankWordTokenizer()\n","    vectorized_data = []\n","    expected = []\n","    for sample in dataset:\n","        tokens = tokenizer.tokenize(sample[1])\n","        sample_vecs = []\n","        for token in tokens:\n","            try:\n","                sample_vecs.append(word_vectors[token])\n","\n","            except KeyError:\n","                pass  # No matching token in the Google w2v vocab\n","            \n","        vectorized_data.append(sample_vecs)\n","\n","    return vectorized_data"],"metadata":{"id":"csENJEGPstNA","executionInfo":{"status":"ok","timestamp":1650966716890,"user_tz":-480,"elapsed":10868,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def collect_expected(dataset):\n","    \"\"\" Peel of the target values from the dataset \"\"\"\n","    expected = []\n","    for sample in dataset:\n","        expected.append(sample[0])\n","    return expected"],"metadata":{"id":"wjkHtXF-s18f","executionInfo":{"status":"ok","timestamp":1650966716892,"user_tz":-480,"elapsed":48,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def pad_trunc(data, maxlen):\n","    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n","    new_data = []\n","\n","    # Create a vector of 0's the length of our word vectors\n","    zero_vector = []\n","    for _ in range(len(data[0][0])):\n","        zero_vector.append(0.0)\n","\n","    for sample in data:\n"," \n","        if len(sample) > maxlen:\n","            temp = sample[:maxlen]\n","        elif len(sample) < maxlen:\n","            temp = sample\n","            additional_elems = maxlen - len(sample)\n","            for _ in range(additional_elems):\n","                temp.append(zero_vector)\n","        else:\n","            temp = sample\n","        new_data.append(temp)\n","    return new_data"],"metadata":{"id":"g08LCaYks2Ei","executionInfo":{"status":"ok","timestamp":1650966716894,"user_tz":-480,"elapsed":46,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","dataset = pre_process_data('./aclImdb/train')\n","vectorized_data = tokenize_and_vectorize(dataset)\n","expected = collect_expected(dataset)\n","\n","split_point = int(len(vectorized_data)*.2)\n","split_point2 = int(len(vectorized_data)*.25)\n","\n","x_train = vectorized_data[:split_point]\n","y_train = expected[:split_point]\n","x_test = vectorized_data[split_point:split_point2]\n","y_test = expected[split_point:split_point2]\n","\n","maxlen = 400\n","batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n","embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n","epochs = 20\n","\n","x_train = pad_trunc(x_train, maxlen)\n","x_test = pad_trunc(x_test, maxlen)\n","\n","x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n","y_train = np.array(y_train)\n","x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n","y_test = np.array(y_test)"],"metadata":{"id":"WIJgPz28s8xf","executionInfo":{"status":"ok","timestamp":1650966779218,"user_tz":-480,"elapsed":62368,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["##**Building LSTM Network (2 stacked layers)**"],"metadata":{"id":"Y9LFr9LOt2Ko"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, LSTM\n","\n","num_neurons = 50\n","\n","print('Build model...')\n","model = Sequential()\n","\n","model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n","model.add(Dropout(.2))\n","\n","model.add(LSTM(num_neurons, return_sequences=True))\n","model.add(Dropout(.2))\n","\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlmFtFrPs85L","executionInfo":{"status":"ok","timestamp":1650963278663,"user_tz":-480,"elapsed":5408,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"3513f648-db00-446b-bbe2-2fcb0cf7e20a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Build model...\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 400, 50)           70200     \n","                                                                 \n"," dropout (Dropout)           (None, 400, 50)           0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 400, 50)           20200     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 400, 50)           0         \n","                                                                 \n"," flatten (Flatten)           (None, 20000)             0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 20001     \n","                                                                 \n","=================================================================\n","Total params: 110,401\n","Trainable params: 110,401\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["##**Training LSTM Network**"],"metadata":{"id":"CfdE9H6Dxu00"}},{"cell_type":"code","source":["model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5Lt4Zess2QK","executionInfo":{"status":"ok","timestamp":1650964926865,"user_tz":-480,"elapsed":1589219,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"22dccc96-c743-4fd7-b281-230042249fe7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","157/157 [==============================] - 84s 504ms/step - loss: 0.5839 - accuracy: 0.6894 - val_loss: 0.4471 - val_accuracy: 0.8136\n","Epoch 2/20\n","157/157 [==============================] - 77s 490ms/step - loss: 0.4575 - accuracy: 0.7940 - val_loss: 0.3667 - val_accuracy: 0.8416\n","Epoch 3/20\n","157/157 [==============================] - 77s 493ms/step - loss: 0.4034 - accuracy: 0.8264 - val_loss: 0.5280 - val_accuracy: 0.7416\n","Epoch 4/20\n","157/157 [==============================] - 78s 500ms/step - loss: 0.3523 - accuracy: 0.8500 - val_loss: 0.3637 - val_accuracy: 0.8480\n","Epoch 5/20\n","157/157 [==============================] - 79s 505ms/step - loss: 0.3079 - accuracy: 0.8716 - val_loss: 0.5340 - val_accuracy: 0.8048\n","Epoch 6/20\n","157/157 [==============================] - 79s 503ms/step - loss: 0.2697 - accuracy: 0.8856 - val_loss: 0.6645 - val_accuracy: 0.7424\n","Epoch 7/20\n","157/157 [==============================] - 78s 499ms/step - loss: 0.2250 - accuracy: 0.9084 - val_loss: 0.4862 - val_accuracy: 0.8192\n","Epoch 8/20\n","157/157 [==============================] - 78s 499ms/step - loss: 0.1860 - accuracy: 0.9264 - val_loss: 0.6078 - val_accuracy: 0.7784\n","Epoch 9/20\n","157/157 [==============================] - 78s 497ms/step - loss: 0.1525 - accuracy: 0.9414 - val_loss: 0.5323 - val_accuracy: 0.8296\n","Epoch 10/20\n","157/157 [==============================] - 78s 495ms/step - loss: 0.1257 - accuracy: 0.9508 - val_loss: 0.5260 - val_accuracy: 0.8424\n","Epoch 11/20\n","157/157 [==============================] - 77s 492ms/step - loss: 0.1017 - accuracy: 0.9586 - val_loss: 0.5966 - val_accuracy: 0.8456\n","Epoch 12/20\n","157/157 [==============================] - 78s 496ms/step - loss: 0.0840 - accuracy: 0.9658 - val_loss: 0.6762 - val_accuracy: 0.8000\n","Epoch 13/20\n","157/157 [==============================] - 78s 496ms/step - loss: 0.0695 - accuracy: 0.9736 - val_loss: 0.9218 - val_accuracy: 0.7832\n","Epoch 14/20\n","157/157 [==============================] - 77s 493ms/step - loss: 0.0583 - accuracy: 0.9802 - val_loss: 1.0231 - val_accuracy: 0.7904\n","Epoch 15/20\n","157/157 [==============================] - 78s 494ms/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.8287 - val_accuracy: 0.8256\n","Epoch 16/20\n","157/157 [==============================] - 77s 493ms/step - loss: 0.0456 - accuracy: 0.9862 - val_loss: 0.7942 - val_accuracy: 0.8456\n","Epoch 17/20\n","157/157 [==============================] - 77s 492ms/step - loss: 0.0378 - accuracy: 0.9864 - val_loss: 0.9037 - val_accuracy: 0.8392\n","Epoch 18/20\n","157/157 [==============================] - 77s 491ms/step - loss: 0.0311 - accuracy: 0.9890 - val_loss: 1.3724 - val_accuracy: 0.7392\n","Epoch 19/20\n","157/157 [==============================] - 78s 495ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.9673 - val_accuracy: 0.8256\n","Epoch 20/20\n","157/157 [==============================] - 78s 496ms/step - loss: 0.0258 - accuracy: 0.9906 - val_loss: 1.1207 - val_accuracy: 0.8024\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f59f64679d0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["##**Saving the LSTM Network**"],"metadata":{"id":"8TsCee1n5S81"}},{"cell_type":"code","source":["model_structure = model.to_json()\n","with open(\"lstm_model1.json\", \"w\") as json_file:\n","  json_file.write(model_structure)\n","model.save_weights(\"lstm_weights_stacked2.h5\")"],"metadata":{"id":"vkk5STkjs2Yu","executionInfo":{"status":"ok","timestamp":1650965195563,"user_tz":-480,"elapsed":525,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from keras.models import model_from_json\n","with open(\"lstm_model1.json\", \"r\") as json_file:\n","  json_string = json_file.read()\n","model = model_from_json(json_string)\n","model.load_weights('lstm_weights1_stacked2.h5')"],"metadata":{"id":"bTcCwvM8smKf","executionInfo":{"status":"ok","timestamp":1650965198203,"user_tz":-480,"elapsed":782,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["##**Prediction**"],"metadata":{"id":"aic9nddKA284"}},{"cell_type":"code","source":["sample_1 = '''I'm hate that the dismal weather that had me down for so long, \n","when will it break! Ugh, when does happiness return?  The sun is blinding and \n","the puffy clouds are too thin.  I can't wait for the weekend.'''\n","\n","# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n","vec_list = tokenize_and_vectorize([(1, sample_1)])\n","\n","# Tokenize returns a list of the data (length 1 here)\n","test_vec_list = pad_trunc(vec_list, maxlen)\n","\n","test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n","\n","print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict(test_vec)))\n","print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1JqLm5vr1sM","executionInfo":{"status":"ok","timestamp":1650965520117,"user_tz":-480,"elapsed":485,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"9298fc72-b40a-4629-f7ca-64a03cc25b61"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample's sentiment, 1 - pos, 2 - neg : [[0.03730413]]\n","Raw output of sigmoid function: [[0.03730413]]\n"]}]},{"cell_type":"markdown","source":["##**Findings**"],"metadata":{"id":"sRn_-pE2N9yL"}},{"cell_type":"markdown","source":["####For 2 stacked layers, the LSTM network had 2 stacked layers with 50 neurons and 2 dropout layers with a rate of 0.2. Without any other further fine tuning other than adding the new layers, the training took 22 minutes and 44 seconds to complete and it finished training with a training accuracy of 99.06% and a loss of 0.0258."],"metadata":{"id":"s2tNRUNLOB2n"}},{"cell_type":"markdown","source":["##**Building LSTM Network (3 stacked layers)**"],"metadata":{"id":"kpF2qQj0PN3x"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, LSTM\n","\n","num_neurons_2 = 50\n","\n","print('Build model...')\n","model2 = Sequential()\n","\n","model2.add(LSTM(num_neurons_2, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n","model2.add(Dropout(.2))\n","\n","model2.add(LSTM(num_neurons_2, return_sequences=True)) # 2nd stacked layer\n","model2.add(Dropout(.2))\n","\n","model2.add(LSTM(num_neurons_2, return_sequences=True)) # 3rd stacked layer\n","model2.add(Dropout(.2))\n","\n","model2.add(Flatten())\n","model2.add(Dense(1, activation='sigmoid'))\n","\n","model2.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n","print(model2.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdF__mtjPUnZ","executionInfo":{"status":"ok","timestamp":1650966809668,"user_tz":-480,"elapsed":7866,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"1f52e6fe-c1b0-48ee-f3ab-28ff9a86e0f2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Build model...\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 400, 50)           70200     \n","                                                                 \n"," dropout (Dropout)           (None, 400, 50)           0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 400, 50)           20200     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 400, 50)           0         \n","                                                                 \n"," lstm_2 (LSTM)               (None, 400, 50)           20200     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 400, 50)           0         \n","                                                                 \n"," flatten (Flatten)           (None, 20000)             0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 20001     \n","                                                                 \n","=================================================================\n","Total params: 130,601\n","Trainable params: 130,601\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["##**Training LSTM Network**"],"metadata":{"id":"SQUpKX2KQHFw"}},{"cell_type":"code","source":["model2.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhyqn5KyPmyh","executionInfo":{"status":"ok","timestamp":1650969199279,"user_tz":-480,"elapsed":2372510,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"dc723c19-2c48-46fa-b798-f55bfa54aeaf"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","157/157 [==============================] - 123s 743ms/step - loss: 0.6150 - accuracy: 0.6714 - val_loss: 0.7204 - val_accuracy: 0.6016\n","Epoch 2/20\n","157/157 [==============================] - 114s 728ms/step - loss: 0.4906 - accuracy: 0.7794 - val_loss: 0.6395 - val_accuracy: 0.6936\n","Epoch 3/20\n","157/157 [==============================] - 117s 743ms/step - loss: 0.4244 - accuracy: 0.8174 - val_loss: 0.6425 - val_accuracy: 0.7224\n","Epoch 4/20\n","157/157 [==============================] - 116s 741ms/step - loss: 0.3817 - accuracy: 0.8368 - val_loss: 0.5850 - val_accuracy: 0.7376\n","Epoch 5/20\n","157/157 [==============================] - 116s 742ms/step - loss: 0.3445 - accuracy: 0.8502 - val_loss: 0.5199 - val_accuracy: 0.7872\n","Epoch 6/20\n","157/157 [==============================] - 117s 748ms/step - loss: 0.3057 - accuracy: 0.8748 - val_loss: 0.4380 - val_accuracy: 0.8152\n","Epoch 7/20\n","157/157 [==============================] - 116s 739ms/step - loss: 0.2746 - accuracy: 0.8862 - val_loss: 0.7065 - val_accuracy: 0.7768\n","Epoch 8/20\n","157/157 [==============================] - 116s 736ms/step - loss: 0.2485 - accuracy: 0.8992 - val_loss: 0.5425 - val_accuracy: 0.7920\n","Epoch 9/20\n","157/157 [==============================] - 115s 736ms/step - loss: 0.2112 - accuracy: 0.9134 - val_loss: 0.5270 - val_accuracy: 0.7984\n","Epoch 10/20\n","157/157 [==============================] - 115s 733ms/step - loss: 0.1786 - accuracy: 0.9272 - val_loss: 0.5702 - val_accuracy: 0.7952\n","Epoch 11/20\n","157/157 [==============================] - 115s 735ms/step - loss: 0.1415 - accuracy: 0.9432 - val_loss: 0.6171 - val_accuracy: 0.7872\n","Epoch 12/20\n","157/157 [==============================] - 115s 735ms/step - loss: 0.1208 - accuracy: 0.9528 - val_loss: 0.7138 - val_accuracy: 0.8064\n","Epoch 13/20\n","157/157 [==============================] - 116s 740ms/step - loss: 0.1022 - accuracy: 0.9610 - val_loss: 0.7636 - val_accuracy: 0.8032\n","Epoch 14/20\n","157/157 [==============================] - 115s 732ms/step - loss: 0.0889 - accuracy: 0.9672 - val_loss: 1.1035 - val_accuracy: 0.7896\n","Epoch 15/20\n","157/157 [==============================] - 115s 735ms/step - loss: 0.0838 - accuracy: 0.9676 - val_loss: 0.9083 - val_accuracy: 0.8096\n","Epoch 16/20\n","157/157 [==============================] - 116s 736ms/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 1.1215 - val_accuracy: 0.7848\n","Epoch 17/20\n","157/157 [==============================] - 116s 737ms/step - loss: 0.0520 - accuracy: 0.9796 - val_loss: 1.0585 - val_accuracy: 0.8064\n","Epoch 18/20\n","157/157 [==============================] - 115s 735ms/step - loss: 0.0487 - accuracy: 0.9818 - val_loss: 1.1377 - val_accuracy: 0.8080\n","Epoch 19/20\n","157/157 [==============================] - 116s 736ms/step - loss: 0.0466 - accuracy: 0.9822 - val_loss: 1.2382 - val_accuracy: 0.7968\n","Epoch 20/20\n","157/157 [==============================] - 117s 747ms/step - loss: 0.0411 - accuracy: 0.9834 - val_loss: 1.1828 - val_accuracy: 0.8016\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fa7ad71bc10>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["##**Saving the LSTM Network**"],"metadata":{"id":"xUl8DjO3QWSy"}},{"cell_type":"code","source":["model_structure = model2.to_json()\n","with open(\"lstm_model1.json\", \"w\") as json_file:\n","  json_file.write(model_structure)\n","model2.save_weights(\"lstm_weights_stacked3.h5\")"],"metadata":{"id":"8dVp1TXcPm7F","executionInfo":{"status":"ok","timestamp":1650969343086,"user_tz":-480,"elapsed":484,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from keras.models import model_from_json\n","with open(\"lstm_model1.json\", \"r\") as json_file:\n","  json_string = json_file.read()\n","model2 = model_from_json(json_string)\n","model2.load_weights('lstm_weights_stacked3.h5')"],"metadata":{"id":"XcCQC30hPnCz","executionInfo":{"status":"ok","timestamp":1650969344458,"user_tz":-480,"elapsed":490,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["##**Prediction**"],"metadata":{"id":"mIQk3DZZQY6M"}},{"cell_type":"code","source":["sample_1 = '''I'm hate that the dismal weather that had me down for so long, \n","when will it break! Ugh, when does happiness return?  The sun is blinding and \n","the puffy clouds are too thin.  I can't wait for the weekend.'''\n","\n","# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n","vec_list = tokenize_and_vectorize([(1, sample_1)])\n","\n","# Tokenize returns a list of the data (length 1 here)\n","test_vec_list = pad_trunc(vec_list, maxlen)\n","\n","test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n","\n","print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model2.predict(test_vec)))\n","print(\"Raw output of sigmoid function: {}\".format(model2.predict(test_vec)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdAZKXVgQVvU","executionInfo":{"status":"ok","timestamp":1650969366917,"user_tz":-480,"elapsed":3366,"user":{"displayName":"Arthur Chin","userId":"01992116258059764651"}},"outputId":"e663a3ae-2386-43c9-b74b-edc7f910fcdc"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample's sentiment, 1 - pos, 2 - neg : [[0.94795674]]\n","Raw output of sigmoid function: [[0.94795674]]\n"]}]},{"cell_type":"markdown","source":["##**Findings**"],"metadata":{"id":"3qhjZ4K5QbKK"}},{"cell_type":"markdown","source":["####For 3 stacked layers, the LSTM network had 3 stacked layers with 50 neurons and 3 dropout layers with a rate of 0.2. Without any other further fine tuning other than adding the new layers, the training took 39 minutes to complete and it finished training with a training accuracy of 98% and a loss of 0.04. This model did well but not as good as the 2 stacked layers model before it."],"metadata":{"id":"EQXLAd3EcbTJ"}}]}